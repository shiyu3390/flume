a1.sources = redis
a1.sinks = k1 
a1.channels = c1

#--------------------------------------redis-------------------------------
a1.sources.redis.type = org.apache.flume.source.sinorail.test.taildir.TaildirSource
a1.sources.redis.channels = c1
a1.sources.redis.skipToEnd = true
a1.sources.redis.positionFile = ./redis.json
a1.sources.redis.filegroups = f1
a1.sources.redis.fileHeader = true
a1.sources.redis.interceptors = i1 i2 i4
a1.sources.redis.interceptors.i1.type = host
a1.sources.redis.interceptors.i1.useIP = false
a1.sources.redis.interceptors.i1.preserveExisting = true
a1.sources.redis.interceptors.i1.hostHeader = hostname
a1.sources.redis.multiline = true
a1.sources.redis.multilinePattern = \\S{1,}\\s+(\\d{1,2}\\s+\\w{3}\\s+\\d{1,2}[:]\\d{1,2}[:]\\d{1,2}[.]\\d{1,5})\\s+([\\s\\S]*)
a1.sources.redis.multilinePatternBelong = previous
a1.sources.redis.multilineMatched = false
a1.sources.redis.multilineEventTimeoutSeconds = 30
a1.sources.redis.multilineMaxBytes = 10485760
a1.sources.redis.multilineMaxLines = 500
a1.sources.redis.interceptors.i2.type = regex_extractor
a1.sources.redis.interceptors.i2.regex = \\S{1,}\\s+(\\d{1,2}\\s+\\w{3}\\s+\\d{1,2}[:]\\d{1,2}[:]\\d{1,2}[.]\\d{1,5})\\s+([\\s\\S]*)
a1.sources.redis.interceptors.i2.serializers = s1 s2 
a1.sources.redis.interceptors.i2.serializers.s1.name = logDate
a1.sources.redis.interceptors.i2.serializers.s2.name = logContent
a1.sources.redis.interceptors.i4.type = formatlogdate

a1.sources.redis.filegroups.f1 = /var/log/redis/redis.log
a1.sources.redis.headers.f1.isCloud = 0
a1.sources.redis.headers.f1.cluster = IAD
a1.sources.redis.headers.f1.module = Redis
a1.sources.redis.headers.f1.role = Redis
a1.sources.redis.headers.f1.dateFormat = dd MMM HH:mm:ss.SSS

a1.channels.c1.type = org.apache.flume.sinorail.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = BrokerLists
a1.channels.c1.kafka.topic = no_cloud_log_default
a1.channels.c1.parseAsFlumeEvent = false
